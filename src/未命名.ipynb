{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "secondary-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimization_openai import OpenAIAdam\n",
    "import math\n",
    "import time\n",
    "from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optimizers\n",
    "from train_process import *\n",
    "import os\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from DataBunch import *\n",
    "from network import BigModel\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from Optim import ScheduledOptim\n",
    "from tqdm import tqdm\n",
    "from Config_File import Config_base\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='CNN text classificer')\n",
    "parser.add_argument('--config', type=str, default='Config_base')\n",
    "parser.add_argument('--batch_size', type=int, default=8, help='batch_size')\n",
    "parser.add_argument('--causal_ratio', type=float,\n",
    "                    default=0.1, help='batch_size')\n",
    "parser.add_argument('--batch_size_test', type=int,\n",
    "                    default=None, help='batch_size_test')\n",
    "parser.add_argument('--epoch', type=int, default=3, help='epoch')\n",
    "parser.add_argument('--dataset', type=str, default='e-snli', help='dataset')\n",
    "parser.add_argument('--grad_loss_func', type=str,\n",
    "                    default='argmax_loss', help='grad_loss_func')\n",
    "parser.add_argument('--saliancy_method', type=str,\n",
    "                    default='compute_saliancy_batch', help='saliancy_method')\n",
    "parser.add_argument('--train_process', type=str,\n",
    "                    default='train_cause_word', help='train_process')\n",
    "parser.add_argument('--model_name_or_path', type=str,\n",
    "                    default='bert-base-uncased', help='model_name_or_path')\n",
    "parser.add_argument('--databunch_method', type=str,\n",
    "                    default='DataBunch_e_snli_marked', help='databunch_method')\n",
    "parser.add_argument('--use_custom_bert', action='store_true',\n",
    "                    default=False, help='use_custom_bert')\n",
    "#parser.add_argument('--tokenizer_name', type=str, default='bert-base-uncased', help='model_name_or_path')\n",
    "parser.add_argument('--load_few', action='store_true',\n",
    "                    default=False, help='load few')\n",
    "parser.add_argument('--grad_clamp', action='store_true',\n",
    "                    default=False, help='grad_clamp')\n",
    "parser.add_argument('--test_mode', action='store_true',\n",
    "                    default=False, help='test_mode or not')\n",
    "parser.add_argument('--no_use_pre_train_parameters', action='store_true', default=False,\n",
    "                    help='no_use_pre_train_parameters or not')\n",
    "\n",
    "args = parser.parse_args(args=['--load_few'])\n",
    "# Config_File.ComputeConfig(args.config)\n",
    "#Config_File.Config = Config_File.ComputeConfig(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exciting-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config_base(args)\n",
    "\n",
    "\n",
    "# from train_augment_process import *\n",
    "# from train_process_MM import *\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "# from Augmentation import Analogy_Auger\n",
    "# from train_process_MM import BigModel_two_bert\n",
    "\n",
    "params_trainset = {'batch_size': config.batch_size,\n",
    "                   'shuffle': False,\n",
    "                   'num_workers': 0}\n",
    "params_testset = {'batch_size': config.batch_size_test,\n",
    "                  'shuffle': False,\n",
    "                  'num_workers': 0}\n",
    "torch.set_printoptions(threshold=10000)\n",
    "\n",
    "\n",
    "def predict(model, test_generator, outputFile):\n",
    "    pred = evaluate(model, criterion, test_generator, False)\n",
    "    with open(outputFile, 'w', encoding='utf-8') as writer:\n",
    "        writer.write('Id,Expected\\n')\n",
    "        for term in pred:\n",
    "            pred_ret = term[1]\n",
    "            pred_label = None\n",
    "            if config.dataset_train == 'RTE':\n",
    "                if pred_ret == 0:\n",
    "                    pred_label = 'not_entailment'\n",
    "                else:\n",
    "                    pred_label = 'entailment'\n",
    "            if config.dataset_train == 'MSRP':\n",
    "                pred_label = pred_ret\n",
    "            if config.dataset_train in ['SNLI', 'mini-SNLI', 'e-snli']:\n",
    "                if pred_ret == 0:\n",
    "                    pred_label = 'neutral'\n",
    "                elif pred_ret == 1:\n",
    "                    pred_label = 'entailment'\n",
    "                else:\n",
    "                    pred_label = 'contradiction'\n",
    "\n",
    "            writer.write('{},{}\\n'.format(term[0], pred_label))\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "record = {}\n",
    "\n",
    "\n",
    "def ReadDataset(args):\n",
    "    global trainset, testset, devset\n",
    "    db_class = globals()[config.databunch_method]\n",
    "    dataset = config.dataset\n",
    "    if config.do_train:\n",
    "        trainset[dataset] = db_class(config, config.train_file_dict[dataset], config.sent_token_dict[dataset], config.label_token_dict[dataset],\n",
    "                                     config.tokenizer, config.sent2_token_dict[dataset],\n",
    "                                     dataset=dataset, id_token=config.id_token_dict[dataset], load_few=args.load_few)\n",
    "        devset[dataset] = db_class(config, config.dev_file_dict[dataset], config.sent_token_dict[dataset],\n",
    "                                   config.label_token_dict[dataset], config.tokenizer, config.sent2_token_dict[dataset],\n",
    "                                   dataset=dataset, id_token=config.id_token_dict[dataset])\n",
    "    if config.do_test:\n",
    "        testset[dataset] = db_class(config, config.test_file_dict[dataset], config.sent_token_dict[dataset],\n",
    "                                    None, config.tokenizer, config.sent2_token_dict[dataset], dataset=dataset, id_token=config.id_token_dict[dataset])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "exciting-style",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start reading file../data/e-snli/esnli_train_1.tsv\n",
      "Loaded Id with size (9998,)\n",
      "Loaded cause_mask with size (9998, 80)\n",
      "Loaded txt with size (9998,)\n",
      "Loaded x_mask with size (9998, 80)\n",
      "Loaded x_pos with size (9998, 80)\n",
      "Loaded x_sent with size (9998, 80)\n",
      "Loaded x_typeid with size (9998, 80)\n",
      "Loaded y with size (9998, 1)\n",
      "Loaded h5 from ../data/e-snli/esnli_train_1.tsv_Pretrain_DataBunch_e_snli_markedbert-base-uncased_loadfew.h5\n",
      "../data/e-snli/esnli_train_1.tsv loaded from h5\n",
      "start reading file../data/e-snli/esnli_dev.tsv\n",
      "Loaded Id with size (9842,)\n",
      "Loaded cause_mask with size (9842, 80)\n",
      "Loaded txt with size (9842,)\n",
      "Loaded x_mask with size (9842, 80)\n",
      "Loaded x_pos with size (9842, 80)\n",
      "Loaded x_sent with size (9842, 80)\n",
      "Loaded x_typeid with size (9842, 80)\n",
      "Loaded y with size (9842, 1)\n",
      "Loaded h5 from ../data/e-snli/esnli_dev.tsv_Pretrain_DataBunch_e_snli_markedbert-base-uncased_loadfew.h5\n",
      "../data/e-snli/esnli_dev.tsv loaded from h5\n"
     ]
    }
   ],
   "source": [
    "trainset = {}\n",
    "testset = {}\n",
    "devset = {}\n",
    "ReadDataset(config)\n",
    "\n",
    "mode_result = []\n",
    "# print(config.__dict__)\n",
    "\n",
    "# model = network.__dict__[Config.big_model](w2id[Config.dataset_train]).to(Config.device)\n",
    "model = globals()[config.big_model](config)\n",
    "if config.multiple_gpu:\n",
    "    model = nn.DataParallel(model)\n",
    "    model = model.cuda()\n",
    "else:\n",
    "    model = model.to(config.device)\n",
    "if config.continue_train:\n",
    "    model.load_state_dict(torch.load(config.model_save_path))\n",
    "    print('continue the training model in {}'.format(config.model_save_path))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "maritime-memphis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Config_File.Config_base at 0x7f5257749790>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pleasant-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = data.DataLoader(\n",
    "    trainset[config.dataset_train], **params_trainset)\n",
    "dev_generator = data.DataLoader(\n",
    "    devset[config.dataset_train], **params_trainset)\n",
    "\n",
    "optimizer = optimizers.__dict__[config.optimizer](model, int(\n",
    "    len(trainset[config.dataset_train]) / params_trainset['batch_size']) * config.epoch)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n",
    "                                            num_training_steps=int(len(train_generator) * config.epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mobile-african",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function train_process_causality.train_cause_word(args, model, optimizer, scheduler, criterion, train_generator, test_generator)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()[args.train_process]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tracked-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "skilled-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accu = 0\n",
    "best_f1 = 0\n",
    "accu_min_train_loss = 0\n",
    "last_update_epoch = 0\n",
    "\n",
    "train_ratios_log, eval_ratios_log = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "necessary-mineral",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_process_causality import *\n",
    "test_generator = dev_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "harmful-nursing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(1/1250) Batch:428.699s|Total:0:07:08|ETA:0:00:00|Loss:0.9181|Loss_ce:1.1076|Grad:7.9031|Grad0:6.0083|top1:0.2500|grad_ratio:1.3154\n",
      "(2/1250) Batch:214.853s|Total:0:07:09|ETA:6 days, 4:44:05|Loss:0.7917|Loss_ce:1.1105|Grad:8.1347|Grad0:4.9461|top1:0.2500|grad_ratio:1.6446\n",
      "(3/1250) Batch:143.549s|Total:0:07:10|ETA:3 days, 2:28:57|Loss:0.8885|Loss_ce:1.1148|Grad:8.2555|Grad0:5.9925|top1:0.2500|grad_ratio:1.3776\n",
      "(4/1250) Batch:107.897s|Total:0:07:11|ETA:2 days, 1:43:27|Loss:3.5734|Loss_ce:1.1137|Grad:28.3100|Grad0:52.9069|top1:0.2500|grad_ratio:0.5351\n",
      "(5/1250) Batch:86.506s|Total:0:07:12|ETA:1 day, 13:20:41|Loss:3.0354|Loss_ce:1.1335|Grad:24.3626|Grad0:43.3812|top1:0.2500|grad_ratio:0.5616\n",
      "(6/1250) Batch:72.246s|Total:0:07:13|ETA:1 day, 5:55:00|Loss:2.6187|Loss_ce:1.1179|Grad:22.3881|Grad0:37.3960|top1:0.2500|grad_ratio:0.5987\n",
      "(7/1250) Batch:62.059s|Total:0:07:14|ETA:1 day, 0:57:54|Loss:2.2929|Loss_ce:1.1293|Grad:21.2709|Grad0:32.9068|top1:0.2500|grad_ratio:0.6464\n",
      "(8/1250) Batch:54.420s|Total:0:07:15|ETA:21:25:40|Loss:2.1031|Loss_ce:1.1154|Grad:20.6088|Grad0:30.4863|top1:0.2500|grad_ratio:0.6760\n",
      "(9/1250) Batch:48.478s|Total:0:07:16|ETA:18:46:30|Loss:1.9816|Loss_ce:1.1283|Grad:19.5553|Grad0:28.0885|top1:0.2500|grad_ratio:0.6962\n",
      "(10/1250) Batch:43.724s|Total:0:07:17|ETA:16:42:42|Loss:1.8805|Loss_ce:1.1268|Grad:18.2399|Grad0:25.7772|top1:0.2500|grad_ratio:0.7076\n",
      "(11/1250) Batch:78.737s|Total:0:14:26|ETA:15:03:38|Loss:1.7694|Loss_ce:1.1349|Grad:17.8551|Grad0:24.2001|top1:0.2500|grad_ratio:0.7378\n",
      "(12/1250) Batch:72.254s|Total:0:14:27|ETA:15:03:15|Loss:1.6225|Loss_ce:1.1448|Grad:18.0362|Grad0:22.8127|top1:0.2500|grad_ratio:0.7906\n",
      "(13/1250) Batch:66.773s|Total:0:14:28|ETA:15:02:32|Loss:1.5089|Loss_ce:1.1485|Grad:18.0317|Grad0:21.6357|top1:0.2500|grad_ratio:0.8334\n",
      "(14/1250) Batch:62.071s|Total:0:14:28|ETA:15:01:47|Loss:1.4379|Loss_ce:1.1496|Grad:18.1442|Grad0:21.0273|top1:0.2500|grad_ratio:0.8629\n",
      "(15/1250) Batch:57.995s|Total:0:14:29|ETA:15:01:04|Loss:1.3516|Loss_ce:1.1493|Grad:18.4929|Grad0:20.5150|top1:0.2500|grad_ratio:0.9014\n",
      "(16/1250) Batch:54.429s|Total:0:14:30|ETA:15:00:19|Loss:1.2716|Loss_ce:1.1461|Grad:19.5306|Grad0:20.7854|top1:0.2500|grad_ratio:0.9396\n",
      "(17/1250) Batch:51.283s|Total:0:14:31|ETA:14:59:35|Loss:1.2256|Loss_ce:1.1457|Grad:23.3774|Grad0:24.1766|top1:0.2500|grad_ratio:0.9669\n",
      "(18/1250) Batch:48.486s|Total:0:14:32|ETA:14:58:51|Loss:1.2299|Loss_ce:1.1451|Grad:32.4060|Grad0:33.2546|top1:0.2500|grad_ratio:0.9745\n",
      "(19/1250) Batch:45.984s|Total:0:14:33|ETA:14:58:07|Loss:1.3298|Loss_ce:1.1421|Grad:39.3791|Grad0:41.2553|top1:0.2500|grad_ratio:0.9545\n",
      "(20/1250) Batch:43.732s|Total:0:14:34|ETA:14:57:23|Loss:1.0816|Loss_ce:1.1411|Grad:46.7675|Grad0:46.1723|top1:0.2500|grad_ratio:1.0129\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5ba94e5b7a10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 train_accuracy, f1, train_loss, train_ratio = evaluate_causal_word(args, model, criterion, train_generator,\n\u001b[1;32m     88\u001b[0m                                                                                    count_limit=3000)\n\u001b[0;32m---> 89\u001b[0;31m                 val_accuracy, f1, val_loss, eval_ratio = evaluate_causal_word(args, model, criterion, test_generator,\n\u001b[0m\u001b[1;32m     90\u001b[0m                                                                               count_limit=3000)\n\u001b[1;32m     91\u001b[0m                 train_ratios_log.append(\n",
      "\u001b[0;32m~/main/src/train_process_causality.py\u001b[0m in \u001b[0;36mevaluate_causal_word\u001b[0;34m(args, model, criterion, test_generator, count_limit)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 loss_gradspred = compute_saliancy(\n\u001b[0m\u001b[1;32m    257\u001b[0m                     args, model, batch_data, retain_graph=False)\n\u001b[1;32m    258\u001b[0m                 \u001b[0;31m#loss_gradspred = nn.LayerNorm(loss_gradspred.size()[1:]).cuda()(loss_gradspred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/main/src/train_process_causality.py\u001b[0m in \u001b[0;36mcompute_saliancy\u001b[0;34m(args, model, batch_data, retain_graph)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_saliancy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaliancy_method\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/main/src/train_process_causality.py\u001b[0m in \u001b[0;36mcompute_saliancy_batch\u001b[0;34m(args, model, batch_data, retain_graph)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_saliancy_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mpred_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/interp/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/interp/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/interp/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/main/src/network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, return_hidden_states)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m#return self.forward_long(temp, input, return_hidden_states)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         outputs= self.pre_emb_model(input_ids=input['x_sent'], attention_mask=input['x_mask'],\n\u001b[0m\u001b[1;32m     55\u001b[0m                                                                   \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                                                                   output_hidden_states=return_hidden_states)\n",
      "\u001b[0;32m~/.conda/envs/interp/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/interp/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    838\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         )\n\u001b[0;32m--> 840\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    841\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/interp/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/interp/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    475\u001b[0m                 )\n\u001b[1;32m    476\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    478\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/interp/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/interp/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     ):\n\u001b[0;32m--> 398\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/interp/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/interp/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     ):\n\u001b[0;32m--> 333\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    334\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/interp/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/interp/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     ):\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mmixed_query_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;31m# If this is instantiated as a cross-attention module, the keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/interp/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/interp/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/interp/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/interp/lib/python3.8/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/interp/lib/python3.8/traceback.py\u001b[0m in \u001b[0;36mformat_list\u001b[0;34m(extracted_list)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mwhose\u001b[0m \u001b[0msource\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \"\"\"\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/interp/lib/python3.8/traceback.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             row.append('  File \"{}\", line {}, in {}\\n'.format(\n\u001b[0m\u001b[1;32m    424\u001b[0m                 frame.filename, frame.lineno, frame.name))\n\u001b[1;32m    425\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epoch):\n",
    "    if (args.early_stop is not None) and (epoch - last_update_epoch > args.early_stop):\n",
    "        break\n",
    "    train_loss = 0\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    ori_losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    prec5 = AverageMeter()\n",
    "    grad_loss = AverageMeter()\n",
    "    grad0_loss = AverageMeter()\n",
    "    priores = AverageMeter()\n",
    "    varies = AverageMeter()\n",
    "    print('')\n",
    "    bar = Bar('Processing', max=len(train_generator))\n",
    "    end = time.time()\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        for iter, batch_data0 in enumerate(train_generator):\n",
    "            batch_data = {}\n",
    "            for k in batch_data0:\n",
    "                v = batch_data0[k]\n",
    "                if isinstance(v, torch.LongTensor) or isinstance(v, torch.FloatTensor):\n",
    "                    batch_data[k] = v.cuda()\n",
    "                else:\n",
    "                    batch_data[k] = v\n",
    "            if 'cause_mask' in batch_data:\n",
    "                cause_mask = batch_data['cause_mask']\n",
    "            else:\n",
    "                cause_mask = torch.zeros(\n",
    "                    batch_data['x_sent'].size()).cuda().int()\n",
    "\n",
    "            data_time.update(time.time() - end)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            loss = 0.0\n",
    "\n",
    "            if True:\n",
    "                loss_gradspred = compute_saliancy(\n",
    "                    args, model, batch_data, retain_graph=True)\n",
    "                #loss_gradspred = nn.LayerNorm(loss_gradspred.size()[1:]).cuda()(loss_gradspred)\n",
    "                '''if iter%max(len(train_generator)//5,1)==0:\n",
    "                    prec5_this = visualize(args, epoch, iter, batch_data, loss_gradspred, write_label='w' if (epoch+iter==0) else 'a')\n",
    "                    prec5.update(prec5_this,batch_data['x_sent'].size(0))'''\n",
    "                loss_g0 = torch.sum(loss_gradspred * (1 - cause_mask) * batch_data['x_mask']) / torch.sum(\n",
    "                    (1 - cause_mask) * batch_data['x_mask'])\n",
    "                if torch.sum(cause_mask) == 0:\n",
    "                    loss_g = loss_g0 * 0.0\n",
    "                else:\n",
    "                    loss_g = torch.sum(\n",
    "                        loss_gradspred * cause_mask) / torch.sum(cause_mask)\n",
    "                grad_loss.update(\n",
    "                    loss_g.item(), batch_data['x_sent'].size(0))\n",
    "                grad0_loss.update(\n",
    "                    loss_g0.item(), batch_data['x_sent'].size(0))\n",
    "                loss_g *= args.causal_ratio\n",
    "                loss_g0 *= args.causal_ratio\n",
    "                if args.grad_clamp:\n",
    "                    loss = -torch.sum(torch.clamp(loss_gradspred, min=1.0) * cause_mask) / torch.sum(cause_mask)\n",
    "                else:\n",
    "                    loss = -loss_g + loss_g0\n",
    "                    #loss = (-torch.sum(loss_gradspred.pow(0.5) * cause_mask) + torch.sum(loss_gradspred.pow(2) * (1-cause_mask))) *args.causal_ratio\n",
    "                #loss = - torch.sum(torch.clamp(torch.abs(loss_gradspred*cause_mask/loss_gradspred_old),max=2.0)) * args.causal_ratio*0.02\n",
    "\n",
    "            local_labels = batch_data['y'].to(args.device).squeeze()\n",
    "\n",
    "            pred_y, deep_repre, seq_repre = model(batch_data)\n",
    "            ce_loss = criterion(\n",
    "                pred_y.reshape(-1, pred_y.shape[-1]), local_labels.reshape(-1))\n",
    "            loss += ce_loss\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            losses.update(loss.item(), batch_data['x_sent'].size(0))\n",
    "            ori_losses.update(ce_loss.item(), batch_data['x_sent'].size(0))\n",
    "\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if iter % 10 == 0 or iter == len(train_generator)-1:\n",
    "                top1.update(get_acc(args, pred_y, local_labels),\n",
    "                            batch_data['y'].size(0))\n",
    "\n",
    "                train_accuracy, f1, train_loss, train_ratio = evaluate_causal_word(args, model, criterion, train_generator,\n",
    "                                                                                   count_limit=3000)\n",
    "                val_accuracy, f1, val_loss, eval_ratio = evaluate_causal_word(args, model, criterion, test_generator,\n",
    "                                                                              count_limit=3000)\n",
    "                train_ratios_log.append(\n",
    "                    (train_ratio, train_accuracy, train_loss))\n",
    "                eval_ratios_log.append(\n",
    "                    (eval_ratio, val_accuracy, val_loss))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            bar.suffix = '({batch}/{size}) Batch:{bt:.3f}s|Total:{total:}|ETA:{eta:}|Loss:{loss:.4f}|Loss_ce:{loss_ce:.4f}|Grad:{grad_loss:.4f}|Grad0:{grad0_loss:.4f}|top1:{accu:.4f}|grad_ratio:{ratio:.4f}'.format(\n",
    "                batch=iter + 1, size=len(train_generator), bt=batch_time.avg,\n",
    "                total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, grad_loss=grad_loss.avg,\n",
    "                grad0_loss=grad0_loss.avg, accu=top1.avg, ratio=grad_loss.avg/grad0_loss.avg, loss_ce=ori_losses.avg)\n",
    "            bar.next()\n",
    "            print(bar.suffix, flush=True)\n",
    "\n",
    "            # evaluate_causal_word(args, model, criterion, test_generator, True)\n",
    "\n",
    "        bar.finish()\n",
    "\n",
    "    print(\"epoch:{} train_loss:{}\".format(\n",
    "        epoch, train_loss / len(train_generator)))\n",
    "\n",
    "    accuracy, f1, val_loss, eval_ratio = evaluate_causal_word(\n",
    "        args, model, criterion, test_generator, True)\n",
    "\n",
    "    if best_accu < accuracy:\n",
    "        best_accu = accuracy\n",
    "        last_update_epoch = epoch\n",
    "        best_f1 = f1\n",
    "        torch.save(model.state_dict(), args.model_save_path)\n",
    "        print('update new model at {}'.format(args.model_save_path))\n",
    "\n",
    "    resultStr = \"mode:{} epoch:{} val_loss:{:.4f} accu:{:.4f}, f1:{}, best accu:{:.4f}, minaccu:{:.4f}\".format(\n",
    "        args.config_name, epoch, val_loss, accuracy, f1, best_accu, accu_min_train_loss)\n",
    "    print(resultStr)\n",
    "\n",
    "with open('ratio{}_log.txt'.format(args.causal_ratio), 'w', encoding='utf-8') as writer:\n",
    "    for i in range(len(train_ratios_log)):\n",
    "        writer.write('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(\n",
    "            train_ratios_log[i][0], train_ratios_log[i][1], train_ratios_log[i][2], eval_ratios_log[i][0], eval_ratios_log[i][1], eval_ratios_log[i][2]))\n",
    "    writer.close()\n",
    "\n",
    "# if args.class_num == 2:\n",
    "#     return best_accu, best_f1\n",
    "# else:\n",
    "#     return best_accu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_result.append(\n",
    "    config.config_name + '\\t' + str(globals()[args.train_process](config, model, optimizer, scheduler, criterion, train_generator, dev_generator)))\n",
    "for mode in mode_result:\n",
    "    print(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.do_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.do_test:\n",
    "    model.load_state_dict(torch.load(config.model_save_path))\n",
    "    test_generator = data.DataLoader(\n",
    "        testset[config.dataset_test], **params_testset)\n",
    "    predict(model, test_generator, config.output_test_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
